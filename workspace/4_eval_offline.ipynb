{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a model offline\n",
    "\n",
    "In this section, we will practice offline evaluation of a model! After you finish this section, you should understand how to:\n",
    "\n",
    "-   evaluate a model on general metrics for its domain\n",
    "-   use human judgement and explainable AI techniques to “sanity check” a model\n",
    "-   evaluate a model with template-based tests\n",
    "-   evaluate a model on slices of interest\n",
    "-   evaluate a model on known failure modes\n",
    "-   and create a test suite out of these evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by loading our trained model and our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "model_path = \"models/food11.pth\"  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "_ = model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "food_11_data_dir = os.getenv(\"FOOD11_DATA_DIR\", \"Food-11\")\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(food_11_data_dir, 'evaluation'), transform=val_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on general metrics for its domain\n",
    "\n",
    "The most basic evaluation will involve general metrics that are relevant to the specific domain.\n",
    "\n",
    "For example, in this case, our model is a classification model, so we will compute accuracy. But, for other models we would consider other metrics -\n",
    "\n",
    "-   If our classification model was highly imbalanced, accuracy would not be appropriate - we would use other classifier metrics.\n",
    "-   If our model was a text generation model, we would consider completely different metrics, e.g. perplexity, ROUGE, BLEU\n",
    "-   etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let’s get the predictions of the model on the held-out test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "dataset_size = len(test_loader.dataset)\n",
    "all_predictions = np.empty(dataset_size, dtype=np.int64)\n",
    "all_labels = np.empty(dataset_size, dtype=np.int64)\n",
    "\n",
    "current_index = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions[current_index:current_index + batch_size] = predicted.cpu().numpy()\n",
    "        all_labels[current_index:current_index + batch_size] = labels.cpu().numpy()\n",
    "        current_index += batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these to compute the overall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "overall_accuracy = (all_predictions == all_labels).sum() / all_labels.shape[0] * 100\n",
    "print(f'Overall Accuracy: {overall_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the per-class accuracy. It would be concerning if our classifier had very low accuracy for some classes, even if it has high accuracy for others. We might set a criteria that e.g. the model we deploy must have a minimum overall accuracy, and then a different minimum per-class accuracy for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "classes = np.array([\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\",\n",
    "    \"Meat\", \"Noodles/Pasta\", \"Rice\", \"Seafood\", \"Soup\", \"Vegetable/Fruit\"])\n",
    "num_classes = classes.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "per_class_correct = np.zeros(num_classes, dtype=np.int32)\n",
    "per_class_total = np.zeros(num_classes, dtype=np.int32)\n",
    "\n",
    "for true_label, pred_label in zip(all_labels, all_predictions):\n",
    "    per_class_total[true_label] += 1\n",
    "    per_class_correct[true_label] += int(true_label == pred_label)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if per_class_total[i] > 0:\n",
    "        acc = per_class_correct[i] / per_class_total[i] * 100\n",
    "        correct_str = f\"{per_class_correct[i]}/{per_class_total[i]}\"\n",
    "        print(f\"{classes[i]:<20} {acc:10.2f}% {correct_str:>20}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can use a confusion matrix to see which classes are most often confused with one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n",
    "for true_label, pred_label in zip(all_labels, all_predictions):\n",
    "    conf_matrix[true_label, pred_label] += 1\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can help us get some insight into our model. For example, we see in the confusion matrix above that “Dessert” and “Dairy product” samples are often confused for one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use human judgement and explainable AI techniques to “sanity check” a model\n",
    "\n",
    "Let’s use our human judgement to better understand some of these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "# Get random sample of Dessert and Dairy product samples that are confused for one another\n",
    "dessert_index = np.where(classes == \"Dessert\")[0][0]\n",
    "dairy_index = np.where(classes == \"Dairy product\")[0][0]\n",
    "\n",
    "confused_indices = [i for i, (t, p) in enumerate(zip(all_labels, all_predictions))\n",
    "                    if (t == dessert_index and p == dairy_index) or (t == dairy_index and p == dessert_index)]\n",
    "\n",
    "sample_indices = np.random.choice(confused_indices, size=min(5, len(confused_indices)), replace=False)\n",
    "\n",
    "# Actually, to make it easier to discuss - we will select specific samples and get those samples from the test loader\n",
    "sample_indices = np.array([404, 927, 496, 435, 667])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "sample_images = []\n",
    "start_idx = 0\n",
    "for images, _ in test_loader:\n",
    "    batch_size = images.size(0)\n",
    "    end_idx = start_idx + batch_size\n",
    "    for idx in sample_indices:\n",
    "        if start_idx <= idx < end_idx:\n",
    "            image = images[idx - start_idx].cpu()\n",
    "            sample_images.append((idx, image))\n",
    "    start_idx = end_idx\n",
    "    if len(sample_images) == len(sample_indices):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "# Visualize those samples (undo the normalization first)\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i, (idx, image) in enumerate(sample_images):\n",
    "    image = image * std[:, None, None] + mean[:, None, None]  # unnormalize\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    image = image.permute(1, 2, 0)  # go from \"channels, height, width\" format to \"height, width, channels\"\n",
    "    plt.subplot(1, len(sample_images), i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"True: {classes[all_labels[idx]]}\\nPred: {classes[all_predictions[idx]]}\\nIndex: {idx}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can better understand some of these errors.\n",
    "\n",
    "For example, sample 496 appears to be ice cream, which is both a dessert and a dairy product. Similarly, sample 927 is cheesecake, which can also be considered both dessert and a dairy product. It is not clear what the instructions to human annotators were, and whether “ice cream” and “cheesecake” are consistently labeled in the training set, or whether some annotators might have labeled these as dessert and others as dairy products.\n",
    "\n",
    "Having identified this problem, we can investigate further and perhaps improve our labeling instructions. We can also re-consider our choice to model this problem as a one-label classification problem, instead of a multi-label classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further “sanity check” a model with explainable AI techniques to better understand which features in an input sample are most influential for it prediction. This can help us find:\n",
    "\n",
    "-   if the model is making predictions based on features that *should* not be useful - this can be a sign of data leakage.\n",
    "-   if the model is making predictions based on features that are associated with membership in a protected class - this can be an indicator of bias or unfairness.\n",
    "-   if the model is not robust to spurious features - we may need to introduce more variety in the training data.\n",
    "\n",
    "Depending on the type of data and model, we can use different explainable AI techniques. For example:\n",
    "\n",
    "-   if we are using a classical ML model on tabular data, we may use measures of feature importance\n",
    "-   if we are using a convolutional neural network on image data, we may use techniques like GradCAM that highlight regions of the image that are most influential\n",
    "-   if we are using a Transformer model on text data, we may use attention weights to show which words the model “focuses” on\n",
    "\n",
    "and more generally, techniques such as SHAP and LIME are applicable to many types of models.\n",
    "\n",
    "Let’s try using GradCAM to highlight the parts of the image that are most influential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# GradCAM setup \n",
    "target_layer = model.features[-1]  \n",
    "cam = GradCAM(model=model, target_layers=[target_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i, (idx, image) in enumerate(sample_images):\n",
    "    input_tensor = (image.clone() - mean[:, None, None]) / std[:, None, None]  # normalize\n",
    "    input_tensor = input_tensor.unsqueeze(0)  # add batch dim\n",
    "\n",
    "    target_category = int(all_predictions[idx])\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(target_category)])\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    image_disp = image * std[:, None, None] + mean[:, None, None]  # unnormalize\n",
    "    image_disp = torch.clamp(image_disp, 0, 1).permute(1, 2, 0).numpy()\n",
    "\n",
    "    visualization = show_cam_on_image(image_disp, grayscale_cam, use_rgb=True)\n",
    "    plt.subplot(1, len(sample_images), i + 1)\n",
    "    plt.imshow(visualization)\n",
    "    plt.title(f\"True: {classes[all_labels[idx]]}\\nPred: {classes[all_predictions[idx]]}\\nIndex: {idx}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have gained additional insight:\n",
    "\n",
    "-   For sample 404, which is a plate of cheese, crackers, and fruit preserves, the model focuses on what appears to be a berry chevre in the corner, and concludes that this is a dessert. If we want “fruit cheese” to be classified as a dairy product, we might consider adding more samples like these to the training data.\n",
    "-   Sample 435 is a cheese plate that is presented in a manner more typical of desserts. The model appears to “focus” on the decoratively plated sauce or spreads, helping us understand why it considers this a dessert; we can similarly consider adding more samples like these to the training data.\n",
    "-   In sample 667, the model appears to “focus” on the fork, rather than the food. This is concerning, and in further evaluation, we will want to make sure that our model will be robust to utensils placed on the image.\n",
    "-   Similarly, in sample 927, the prediction seems to be based on a plant in the background instead of the food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a model with template-based tests\n",
    "\n",
    "In the previous part, we saw indications that the model may focus on spurious features of the image - like the fork - when we would prefer for it to focus on the actual food item.\n",
    "\n",
    "Let’s design a template-based test to help evaluate the extent to which:\n",
    "\n",
    "-   the model is robust to perturbations in the input that should *not* change the label - for example, if the background changes, or a utensil is placed alongside the food.\n",
    "-   and the model *does* change its label if the background or utensils stay the same, but the actual food item changes.\n",
    "\n",
    "Inside the “templates” directory, we have prepared some images as follows:\n",
    "\n",
    "-   background images, representing the context in which a food item may be found\n",
    "-   “extra” items with transparent background, that may typically be found in that context. These can be overlaid on top of the background images.\n",
    "-   and “food” items (organized by class) with transparent background. These can also be overlaid on top of the background images.\n",
    "\n",
    "Let’s look at these now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "TEMPLATE_DIR = \"templates\"\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 6))\n",
    "\n",
    "# Random food class\n",
    "food_dir = os.path.join(TEMPLATE_DIR, \"food\")\n",
    "food_classes = [d for d in os.listdir(food_dir) if os.listdir(os.path.join(food_dir, d))]\n",
    "random_class = random.choice(food_classes)\n",
    "food_images = random.sample(os.listdir(os.path.join(food_dir, random_class)), 3)\n",
    "food_paths = [os.path.join(food_dir, random_class, f) for f in food_images]\n",
    "\n",
    "for i, path in enumerate(food_paths):\n",
    "    axes[0, i].imshow(Image.open(path))\n",
    "    axes[0, i].set_title(f\"Food ({random_class})\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "# Backgrounds\n",
    "bg_dir = os.path.join(TEMPLATE_DIR, \"background\")\n",
    "bg_images = random.sample(os.listdir(bg_dir), 3)\n",
    "bg_paths = [os.path.join(bg_dir, f) for f in bg_images]\n",
    "\n",
    "for i, path in enumerate(bg_paths):\n",
    "    axes[1, i].imshow(Image.open(path))\n",
    "    axes[1, i].set_title(\"Background\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "# Extras\n",
    "extra_dir = os.path.join(TEMPLATE_DIR, \"extras\")\n",
    "extra_images = random.sample(os.listdir(extra_dir), 3)\n",
    "extra_paths = [os.path.join(extra_dir, f) for f in extra_images]\n",
    "\n",
    "for i, path in enumerate(extra_paths):\n",
    "    axes[2, i].imshow(Image.open(path))\n",
    "    axes[2, i].set_title(\"Extra\")\n",
    "    axes[2, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, here is a function that can compose an image out of a background, a food item, and (optionally) an extra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "def compose_image(food_path, bg_path=None, extra_path=None):\n",
    "\n",
    "    food = Image.open(food_path).convert(\"RGBA\")\n",
    "\n",
    "    if bg_path:\n",
    "        bg = Image.open(bg_path).convert(\"RGBA\")\n",
    "    else:\n",
    "        bg = Image.new(\"RGBA\", food.size, (255, 255, 255, 255))\n",
    "\n",
    "    bg_w, bg_h = bg.size\n",
    "    y_offset = int(bg_h * 0.05)\n",
    "            \n",
    "    food_scale = 0.5\n",
    "    food = food.resize((int(bg_w * food_scale), int(bg_h * food_scale)))\n",
    "    \n",
    "    fd_w, fd_h = food.size\n",
    "\n",
    "    if extra_path:\n",
    "        \n",
    "        extra_scale = 0.35\n",
    "        extra = Image.open(extra_path).convert(\"RGBA\")\n",
    "        extra = extra.resize((int(bg_w * extra_scale), int(bg_h * extra_scale)))\n",
    "        ex_w, ex_h = extra.size\n",
    "        bg.paste(extra, (bg_w - ex_w, bg_h - ex_h - y_offset), extra)\n",
    "        \n",
    "    bg.paste(food, ((bg_w - fd_w) // 2, bg_h - fd_h - y_offset), food)\n",
    "\n",
    "    return bg.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try composing:\n",
    "\n",
    "-   a food item\n",
    "-   the same food item and background, with a background and an “extra”\n",
    "-   the same food item, with a different background and different “extra”\n",
    "\n",
    "These should all have the same prediction. Notice that this is a test we could even run on an *unlabeled* food item - we don’t need to know the actual class (although we do, in this case).\n",
    "\n",
    "We can also try:\n",
    "\n",
    "-   a different food item from the same class, on the same background - this should have the same predicted label\n",
    "-   and a food item from a different class, on the same background - this should have a different predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "imgs = {\n",
    "    'original_image': compose_image('templates/food/09/001.png'),\n",
    "    'composed_bg1_extra1': compose_image('templates/food/09/001.png', 'templates/background/001.jpg', 'templates/extras/spoon.png'),\n",
    "    'composed_bg2_extra2': compose_image('templates/food/09/001.png', 'templates/background/002.jpg', 'templates/extras/fork.png'),\n",
    "    'composed_same_class': compose_image('templates/food/09/002.png', 'templates/background/001.jpg'),\n",
    "    'composed_diff_class': compose_image('templates/food/05/002.png', 'templates/background/001.jpg')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, let’s look at these examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "fig, axes = plt.subplots(1, 5, figsize=(14, 3))\n",
    "\n",
    "for ax, key in zip(axes, imgs.keys()):\n",
    "    ax.imshow(imgs[key].resize((224,224)).crop((16, 16, 224, 224)))\n",
    "    ax.set_title(f\"{key}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the predictions of the model for these samples, as well as the GradCAM output -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "def predict(model, image, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    model.eval()\n",
    "    image_tensor = val_test_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        output = model(image_tensor)\n",
    "        return output.argmax(dim=1).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "\n",
    "for i, key in enumerate(imgs.keys()):\n",
    "    image_np = np.array(imgs[key].resize((224, 224))).astype(dtype=np.float32) / 255.0\n",
    "    pred = predict(model, imgs[key])\n",
    "\n",
    "    input_tensor = val_test_transform(imgs[key]).unsqueeze(0)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred)])[0]\n",
    "    vis = show_cam_on_image(image_np, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    axes[0, i].imshow(imgs[key].resize((224, 224)))\n",
    "    axes[0, i].set_title(f\"{key}\\nPredicted: {pred} ({classes[pred]})\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    axes[1, i].imshow(vis)\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seemed OK - but let’s try it for a different combination of food item, background, and “extra” item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "imgs = {\n",
    "    'original_image': compose_image('templates/food/10/001.png'),\n",
    "    'composed_bg1_extra1': compose_image('templates/food/10/001.png', 'templates/background/003.jpg', 'templates/extras/plastic_fork.png'),\n",
    "    'composed_bg2_extra2': compose_image('templates/food/10/001.png', 'templates/background/002.jpg', 'templates/extras/fork.png'),\n",
    "    'composed_same_class': compose_image('templates/food/10/002.png', 'templates/background/003.jpg'),\n",
    "    'composed_diff_class': compose_image('templates/food/05/003.png', 'templates/background/003.jpg')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and repeat the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "\n",
    "for i, key in enumerate(imgs.keys()):\n",
    "    image_np = np.array(imgs[key].resize((224, 224))).astype(dtype=np.float32) / 255.0\n",
    "    pred = predict(model, imgs[key])\n",
    "\n",
    "    input_tensor = val_test_transform(imgs[key]).unsqueeze(0)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred)])[0]\n",
    "    vis = show_cam_on_image(image_np, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    axes[0, i].imshow(imgs[key].resize((224, 224)))\n",
    "    axes[0, i].set_title(f\"{key}\\nPredicted: {pred} ({classes[pred]})\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    axes[1, i].imshow(vis)\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that our model is not nearly as robust as we might want it to be. We can see that some perturbations that should *not* change the model output do, and the model sometimes focuses on spurious items like the background or extra items that happen to be alongside the food in the image.\n",
    "\n",
    "This type of test can be automated - we can collect a large number of food items, background, and “extras” and systematically evaluate this robustness as part of a test suite. We’ll get to that a little bit later, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a model on slices of interest\n",
    "\n",
    "Next, let’s evaluate our model on slices of interest. This will help us understand:\n",
    "\n",
    "-   if there is a fairness or bias issue in our model - if we evaluate the performance of the model on different groups, it will help us identify potential unfairness.\n",
    "-   if our model is accurate enough on high-priority use cases.\n",
    "\n",
    "For example, our model is reasonably accurate on “Dessert” samples. However, looking at some samples from this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "dessert_index = np.where(classes == \"Dessert\")[0][0]\n",
    "dessert_images = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    for img, label in zip(images, labels):\n",
    "        if label.item() == dessert_index:\n",
    "            dessert_images.append(img)\n",
    "        if len(dessert_images) == 20:\n",
    "            break\n",
    "    if len(dessert_images) == 20:\n",
    "        break\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, figsize=(10, 10))\n",
    "for ax, img in zip(axes.flat, dessert_images):\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "    ax.set_title(\"Dessert\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that these are mostly Western-style desserts.\n",
    "\n",
    "To evaluate whether our model is similarly effective at classifying food items from other cuisines, we might compile one or more test suites with non-Western food items. The images in the “indian_dessert” directory are also desserts -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "\n",
    "dessert_dir = \"indian_dessert\"\n",
    "dessert_images = random.sample(os.listdir(dessert_dir), 5)\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 3))\n",
    "\n",
    "for ax, img_name in zip(axes, dessert_images):\n",
    "    path = os.path.join(dessert_dir, img_name)\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    pred = predict(model, image)\n",
    "    ax.imshow(image.resize((224, 224)).crop((16, 16, 224, 224)))\n",
    "    ax.set_title(f\"Predicted: {pred}\\n({classes[pred]})\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but our model has much less predictive accuracy on *these* dessert samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a model on known failure modes\n",
    "\n",
    "We might also consider evaluating a model on “known” failure modes - if a model has previously failed on a specific type of image, especially in a high-profile way, we will want to set up a test so that future versions of the model will be evaluated against this type of failure.\n",
    "\n",
    "For example: Suppose that there is a trend on social media of making cakes that look like other items. This has been a high-profile failure for GourmetGram in the past, when users upload e.g. a photo of a cake that looks like (for example…) a stick of butter, and it is tagged as “Dairy product” instead of “Dessert”.\n",
    "\n",
    "We could compile a set of test images related to this failure mode, and set it up as a separate test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the photos in the “cake_looks_like” directory are actually photos of cake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "cake_dir = \"cake_looks_like\"\n",
    "cake_images = random.sample(os.listdir(cake_dir), 5)\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 3))\n",
    "\n",
    "for ax, img_name in zip(axes, cake_images):\n",
    "    path = os.path.join(cake_dir, img_name)\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    pred = predict(model, image)\n",
    "    ax.imshow(image.resize((224, 224)).crop((16, 16, 224, 224)))\n",
    "    ax.set_title(f\"{img_name}, Predicted: {pred}\\n({classes[pred]})\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but, our model has a much lower accuracy on these samples than it did overall or on the general “Dessert” category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test suite\n",
    "\n",
    "Finally, let’s create a non-interactive test suite that we can run each time we re-train a model. We’ll use `pytest` as a unit test framework. Our test scripts are in “tests”. Open these Python files to review then.\n",
    "\n",
    "Inside our “conftest.py”, we have some functions that are “fixtures”. These provide some shared context for the tests. We have defined:\n",
    "\n",
    "-   `model`, which loads the saved model\n",
    "-   `test_data`, which sets up the test data loader\n",
    "-   `predictions`, which gets the model predictions on the test set\n",
    "-   and we also define `predict` and `transform` functions so that we can use them throughout the test suite without having to repeat them in multiple files.\n",
    "\n",
    "Then, we can pass `model`, `test_data`, and/or `predictions` to any of our test functions - these fixture functions will run only *once* and then provide their values to all test cases in a session. Similarly, we can pass `predict` to a test function and it will then be able to call our already-defined predict function.\n",
    "\n",
    "Next, we have the test functions, which mirror what we have done in this notebook - but now, we have defined criteria for passing. A test function raises an exception if the criteria for passing are not met:\n",
    "\n",
    "-   `test_overall_accuracy` passes if the overall accuracy is greater than 85%\n",
    "-   `test_per_class_accuracy` passes if accuracy is greater than 75% for every individual class\n",
    "-   `test_template_permutations` generates combinations of “composed” images using the template: food item, extra item, and background. It passes if 80% are predicted correctly.\n",
    "-   `test_cake_looks_like_accuracy` and `test_indian_dessert_accuracy` each pass if accuracy is greater than 60% on the corresponding sets of images.\n",
    "\n",
    "Of course, in a realistic setting, we would have a more comprehensive series of tests (e.g. we would have many more templates; we would evaluate the model on other forms of non-Western cuisine and not only Indian desserts, etc.). This is just a demonstration of how these tests would be automated.\n",
    "\n",
    "Once we have defined these tests, we can run our test suite with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "!pytest --verbose --tb=no tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "-   `--verbose` will report the result of every test, not just the ones that fail\n",
    "-   `--tb=no` says not to print a traceback for every failed test\n",
    "-   and `pytest` will automatically discover and run every function that starts with `test_` in this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only scratched the surface with `pytest`. For example, we can re-run the tests that failed in the last run with `--lf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "!pytest --verbose --lf --tb=no tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we can run only the tests from a particular test file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in jupyter container on node-eval-offline\n",
    "!pytest --verbose --tb=no tests/test_food11_test_cases.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about its capabilities in its [documentation](https://docs.pytest.org/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "When you are finished with this section - save and then download the fully executed notebook from the Jupyter container environment for later reference. (Note: because it is an executable file, and you are downloading it from a site that is not secured with HTTPS, you may have to explicitly confirm the download in some browsers.)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 }
}
